{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: ./TrainingData\\L10_Train.csv with shape (91340, 8)\n",
      "Loaded file: ./TrainingData\\L11_Train.csv with shape (56968, 8)\n",
      "Loaded file: ./TrainingData\\L12_Train.csv with shape (82911, 8)\n",
      "Loaded file: ./TrainingData\\L13_Train.csv with shape (69506, 8)\n",
      "Loaded file: ./TrainingData\\L14_Train.csv with shape (66151, 8)\n",
      "Loaded file: ./TrainingData\\L15_Train.csv with shape (85240, 8)\n",
      "Loaded file: ./TrainingData\\L16_Train.csv with shape (83258, 8)\n",
      "Loaded file: ./TrainingData\\L17_Train.csv with shape (96642, 8)\n",
      "Loaded file: ./TrainingData\\L18_Train.csv with shape (2255, 8)\n",
      "Loaded file: ./TrainingData\\L19_Train.csv with shape (12040, 8)\n",
      "Loaded file: ./TrainingData\\L1_Train.csv with shape (101673, 8)\n",
      "Loaded file: ./TrainingData\\L20_Train.csv with shape (11237, 8)\n",
      "Loaded file: ./TrainingData\\L21_Train.csv with shape (22671, 8)\n",
      "Loaded file: ./TrainingData\\L22_Train.csv with shape (11699, 8)\n",
      "Loaded file: ./TrainingData\\L23_Train.csv with shape (13872, 8)\n",
      "Loaded file: ./TrainingData\\L24_Train.csv with shape (10360, 8)\n",
      "Loaded file: ./TrainingData\\L2_Train.csv with shape (87080, 8)\n",
      "Loaded file: ./TrainingData\\L3_Train.csv with shape (55380, 8)\n",
      "Loaded file: ./TrainingData\\L4_Train.csv with shape (56900, 8)\n",
      "Loaded file: ./TrainingData\\L5_Train.csv with shape (55157, 8)\n",
      "Loaded file: ./TrainingData\\L6_Train.csv with shape (52559, 8)\n",
      "Loaded file: ./TrainingData\\L7_Train.csv with shape (43518, 8)\n",
      "Loaded file: ./TrainingData\\L8_Train.csv with shape (116999, 8)\n",
      "Loaded file: ./TrainingData\\L9_Train.csv with shape (89612, 8)\n",
      "Combined DataFrame shape: (1375028, 8)\n",
      "High-Cardinality Columns (1): ['DateTime']\n",
      "Low-Cardinality Columns (0): []\n",
      "Shape after one-hot encoding low-cardinality columns: (1375028, 6)\n",
      "Applied target encoding on high-cardinality columns.\n",
      "Training set shape: (1100022, 6)\n",
      "Validation set shape: (275006, 6)\n",
      "Training and evaluating model: LightGBM\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1424\n",
      "[LightGBM] [Info] Number of data points in the train set: 880017, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 248.579282\n",
      "  Fold 1 MAE: 20.1341\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1419\n",
      "[LightGBM] [Info] Number of data points in the train set: 880017, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 248.712798\n",
      "  Fold 2 MAE: 20.1016\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1426\n",
      "[LightGBM] [Info] Number of data points in the train set: 880018, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 248.731313\n",
      "  Fold 3 MAE: 20.0189\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1418\n",
      "[LightGBM] [Info] Number of data points in the train set: 880018, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 248.576261\n",
      "  Fold 4 MAE: 20.0955\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1419\n",
      "[LightGBM] [Info] Number of data points in the train set: 880018, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 248.708879\n",
      "  Fold 5 MAE: 20.1870\n",
      "Average MAE for LightGBM: 20.1074\n",
      "\n",
      "Training and evaluating model: XGBoost\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在目前儲存格或上一個儲存格中執行程式碼時，Kernel 已損毀。\n",
      "\u001b[1;31m請檢閱儲存格中的程式碼，找出失敗的可能原因。\n",
      "\u001b[1;31m如需詳細資訊，請按一下<a href='https://aka.ms/vscodeJupyterKernelCrash'>這裡</a>。\n",
      "\u001b[1;31m如需詳細資料，請檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>。"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "from glob import glob\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import StackingRegressor, VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define file paths (Update these paths accordingly)\n",
    "TRAIN_DATA_DIR = './TrainingData'  # Replace with your training data directory path\n",
    "SUBMISSION_PATH = './submission.csv'           # Replace with your desired submission path\n",
    "\n",
    "# Define constants\n",
    "TARGET_COLUMN = 'Power(mW)'  # Replace with your target column name\n",
    "ID_COLUMN = 'LocationCode'                     # Replace with your ID column name (if available)\n",
    "N_SPLITS = 5                         # Number of folds for cross-validation\n",
    "RANDOM_STATE = 42                    # Random seed for reproducibility\n",
    "\n",
    "# Function to load and concatenate multiple CSV files\n",
    "def load_and_concatenate_csvs(data_dir):\n",
    "    \"\"\"\n",
    "    Load all CSV files from the specified directory and concatenate them into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        data_dir (str): Path to the directory containing CSV files.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame containing all training data.\n",
    "    \"\"\"\n",
    "    all_files = glob(os.path.join(data_dir, \"*.csv\"))\n",
    "    if not all_files:\n",
    "        print(f\"No CSV files found in directory: {data_dir}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    list_df = []\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            list_df.append(df)\n",
    "            print(f\"Loaded file: {file} with shape {df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "    \n",
    "    combined_df = pd.concat(list_df, ignore_index=True)\n",
    "    print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "    return combined_df\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the combined DataFrame.\n",
    "    - Handle missing values\n",
    "    - Encode categorical variables appropriately\n",
    "    - Feature scaling\n",
    "    - Split into training and validation sets\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Combined DataFrame containing all training data.\n",
    "    \n",
    "    Returns:\n",
    "        X_train (pd.DataFrame): Preprocessed training features.\n",
    "        X_val (pd.DataFrame): Preprocessed validation features.\n",
    "        y_train (pd.Series): Training target.\n",
    "        y_val (pd.Series): Validation target.\n",
    "        ids_train (pd.Series): Training IDs (if available).\n",
    "        ids_val (pd.Series): Validation IDs (if available).\n",
    "        scaler (StandardScaler): Fitted scaler object.\n",
    "        encoder (TargetEncoder): Fitted encoder object.\n",
    "    \"\"\"\n",
    "    # Identify if ID_COLUMN exists and separate it\n",
    "    if ID_COLUMN in df.columns:\n",
    "        ids = df[ID_COLUMN]\n",
    "        df = df.drop(columns=[ID_COLUMN])\n",
    "    else:\n",
    "        ids = None\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[TARGET_COLUMN])\n",
    "    y = df[TARGET_COLUMN]\n",
    "\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Handle missing values\n",
    "    for col in numerical_cols:\n",
    "        median = X[col].median()\n",
    "        X[col].fillna(median, inplace=True)\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        mode = X[col].mode()[0]\n",
    "        X[col].fillna(mode, inplace=True)\n",
    "\n",
    "    # Identify high and low cardinality categorical columns\n",
    "    high_cardinality_cols = [col for col in categorical_cols if X[col].nunique() > 100]\n",
    "    low_cardinality_cols = [col for col in categorical_cols if X[col].nunique() <= 100]\n",
    "\n",
    "    print(f\"High-Cardinality Columns ({len(high_cardinality_cols)}): {high_cardinality_cols}\")\n",
    "    print(f\"Low-Cardinality Columns ({len(low_cardinality_cols)}): {low_cardinality_cols}\")\n",
    "\n",
    "    # Handle low-cardinality categorical columns with one-hot encoding\n",
    "    X_encoded = pd.get_dummies(X, columns=low_cardinality_cols, drop_first=True)\n",
    "    print(f\"Shape after one-hot encoding low-cardinality columns: {X_encoded.shape}\")\n",
    "\n",
    "    # Handle high-cardinality categorical columns with target encoding\n",
    "    if high_cardinality_cols:\n",
    "        encoder = TargetEncoder(cols=high_cardinality_cols)\n",
    "        X_encoded[high_cardinality_cols] = encoder.fit_transform(X_encoded[high_cardinality_cols], y)\n",
    "        print(f\"Applied target encoding on high-cardinality columns.\")\n",
    "    else:\n",
    "        encoder = None\n",
    "\n",
    "    # Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_encoded[numerical_cols] = scaler.fit_transform(X_encoded[numerical_cols])\n",
    "\n",
    "    # Optional: Feature Engineering can be added here\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_encoded, y, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # If IDs were present, split them accordingly\n",
    "    if ids is not None:\n",
    "        ids_train, ids_val = train_test_split(\n",
    "            ids, test_size=0.2, random_state=RANDOM_STATE\n",
    "        )\n",
    "    else:\n",
    "        ids_train, ids_val = None, None\n",
    "\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Validation set shape: {X_val.shape}\")\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, ids_train, ids_val, scaler, encoder\n",
    "\n",
    "# Function to define individual models\n",
    "def define_models():\n",
    "    \"\"\"\n",
    "    Define all individual models to be compared.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of model names and their corresponding instances.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # LightGBM Regressor\n",
    "    models['LightGBM'] = lgb.LGBMRegressor(\n",
    "        objective='regression',\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=1000,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # XGBoost Regressor\n",
    "    models['XGBoost'] = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # CatBoost Regressor\n",
    "    models['CatBoost'] = CatBoostRegressor(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_seed=RANDOM_STATE,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Define a simpler and optimized MLPRegressor\n",
    "    models['MLPRegressor'] = MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),  # Reduced complexity\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.0001,\n",
    "        batch_size='auto',\n",
    "        learning_rate='adaptive',\n",
    "        max_iter=200,  # Limited iterations\n",
    "        tol=1e-3,       # Increased tolerance\n",
    "        early_stopping=True,  # Enable early stopping\n",
    "        validation_fraction=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=True  # Enable verbose to monitor progress\n",
    "    )\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Function to define ensemble models\n",
    "def define_ensemble_models():\n",
    "    \"\"\"\n",
    "    Define Stacking and Voting Ensemble models.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of ensemble model names and their corresponding instances.\n",
    "    \"\"\"\n",
    "    # Define base learners\n",
    "    base_learners = [\n",
    "        ('lgb', lgb.LGBMRegressor(n_estimators=500, learning_rate=0.05, random_state=RANDOM_STATE)),\n",
    "        ('xgb', xgb.XGBRegressor(n_estimators=500, learning_rate=0.05, random_state=RANDOM_STATE)),\n",
    "        ('cat', CatBoostRegressor(iterations=500, learning_rate=0.05, depth=6, loss_function='MAE',\n",
    "                                 eval_metric='MAE', random_seed=RANDOM_STATE, verbose=0))\n",
    "    ]\n",
    "    \n",
    "    # Define Stacking Regressor with Linear Regression as meta-learner\n",
    "    stacking_regressor = StackingRegressor(\n",
    "        estimators=base_learners,\n",
    "        final_estimator=LinearRegression(),\n",
    "        passthrough=True,\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Define Voting Regressor (soft voting via averaging)\n",
    "    voting_regressor = VotingRegressor(\n",
    "        estimators=base_learners,\n",
    "        weights=[1, 1, 1]\n",
    "    )\n",
    "    \n",
    "    ensemble_models = {\n",
    "        'StackingRegressor': stacking_regressor,\n",
    "        'VotingRegressor': voting_regressor\n",
    "    }\n",
    "    \n",
    "    return ensemble_models\n",
    "\n",
    "# Function to define Neural Network using TensorFlow Keras\n",
    "def define_keras_model(input_dim):\n",
    "    \"\"\"\n",
    "    Define and compile a Keras Neural Network model.\n",
    "    \n",
    "    Parameters:\n",
    "        input_dim (int): Number of input features.\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    return model\n",
    "\n",
    "# Function to evaluate models using K-Fold Cross-Validation\n",
    "def evaluate_models(models, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Evaluate each model using K-Fold Cross-Validation and return MAE scores.\n",
    "    \n",
    "    Parameters:\n",
    "        models (dict): Dictionary of model names and instances.\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of model names and their average MAE scores.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    mae_scores = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training and evaluating model: {name}\")\n",
    "        fold_mae = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "            X_tr, X_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_tr, y_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            \n",
    "            if name == 'CatBoost':\n",
    "                model.fit(X_tr, y_tr, eval_set=(X_fold, y_fold), verbose=0)\n",
    "            else:\n",
    "                model.fit(X_tr, y_tr)\n",
    "            \n",
    "            preds = model.predict(X_fold)\n",
    "            mae = mean_absolute_error(y_fold, preds)\n",
    "            fold_mae.append(mae)\n",
    "            print(f\"  Fold {fold+1} MAE: {mae:.4f}\")\n",
    "        \n",
    "        avg_mae = np.mean(fold_mae)\n",
    "        mae_scores[name] = avg_mae\n",
    "        print(f\"Average MAE for {name}: {avg_mae:.4f}\\n\")\n",
    "    \n",
    "    return mae_scores\n",
    "\n",
    "# Function to evaluate Keras Neural Network separately\n",
    "def evaluate_keras_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Evaluate Keras Neural Network using K-Fold Cross-Validation and return MAE scores.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing the Keras NN's MAE score.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    fold_mae = []\n",
    "    \n",
    "    print(\"Training and evaluating model: Keras Neural Network\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        X_tr, X_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        # Define the model\n",
    "        input_dim = X_tr.shape[1]\n",
    "        model = define_keras_model(input_dim)\n",
    "        \n",
    "        # Define early stopping\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_tr, y_tr,\n",
    "            validation_data=(X_fold, y_fold),\n",
    "            epochs=1000,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        preds = model.predict(X_fold).flatten()\n",
    "        mae = mean_absolute_error(y_fold, preds)\n",
    "        fold_mae.append(mae)\n",
    "        print(f\"  Fold {fold+1} MAE: {mae:.4f}\")\n",
    "    \n",
    "    avg_mae = np.mean(fold_mae)\n",
    "    print(f\"Average MAE for Keras Neural Network: {avg_mae:.4f}\\n\")\n",
    "    \n",
    "    return {'KerasNN': avg_mae}\n",
    "\n",
    "# Function to train the best model on the entire training data and make predictions\n",
    "def train_and_predict_best_model(best_model_name, models, X_train, y_train, X_val):\n",
    "    \"\"\"\n",
    "    Retrain the best model on the entire training data and make predictions on the validation set.\n",
    "    \n",
    "    Parameters:\n",
    "        best_model_name (str): Name of the best-performing model.\n",
    "        models (dict): Dictionary of individual and ensemble models.\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target.\n",
    "        X_val (pd.DataFrame): Validation features.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray or list: Predictions on the validation set.\n",
    "    \"\"\"\n",
    "    print(f\"Retraining the best model: {best_model_name} on the entire training data...\")\n",
    "    \n",
    "    if best_model_name in models:\n",
    "        best_model = models[best_model_name]\n",
    "        if best_model_name == 'CatBoost':\n",
    "            best_model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=0)\n",
    "        else:\n",
    "            best_model.fit(X_train, y_train)\n",
    "        predictions = best_model.predict(X_val)\n",
    "    elif best_model_name in ensemble_models:\n",
    "        best_model = ensemble_models[best_model_name]\n",
    "        best_model.fit(X_train, y_train)\n",
    "        predictions = best_model.predict(X_val)\n",
    "    elif best_model_name == 'KerasNN':\n",
    "        # Define and train Keras model on entire training data\n",
    "        input_dim = X_train.shape[1]\n",
    "        best_model = define_keras_model(input_dim)\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=50, restore_best_weights=True)\n",
    "        best_model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=1000,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        predictions = best_model.predict(X_val).flatten()\n",
    "    else:\n",
    "        print(\"Unexpected model selection.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Optional: Clip predictions to a reasonable range if necessary\n",
    "    predictions = np.clip(predictions, a_min=0, a_max=None)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Main function to execute the comparison\n",
    "def main():\n",
    "    # Load and concatenate multiple CSV files\n",
    "    combined_df = load_and_concatenate_csvs(TRAIN_DATA_DIR)\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train, X_val, y_train, y_val, ids_train, ids_val, scaler, encoder = preprocess_data(combined_df)\n",
    "    \n",
    "    # Define individual models\n",
    "    models = define_models()\n",
    "    \n",
    "    # Define ensemble models\n",
    "    ensemble_models = define_ensemble_models()\n",
    "    \n",
    "    # Combine all models into a single dictionary\n",
    "    all_models = {**models, **ensemble_models}\n",
    "    \n",
    "    # Evaluate individual and ensemble models\n",
    "    mae_scores = evaluate_models(all_models, X_train, y_train)\n",
    "    \n",
    "    # Evaluate Keras Neural Network separately\n",
    "    keras_mae = evaluate_keras_model(X_train, y_train)\n",
    "    \n",
    "    # Combine all MAE scores\n",
    "    mae_scores.update(keras_mae)\n",
    "    \n",
    "    # Display MAE comparison\n",
    "    print(\"=== Model Performance Comparison ===\")\n",
    "    for model_name, mae in sorted(mae_scores.items(), key=lambda x: x[1]):\n",
    "        print(f\"{model_name}: MAE = {mae:.4f}\")\n",
    "    \n",
    "    # Identify the best model\n",
    "    best_model_name = min(mae_scores, key=mae_scores.get)\n",
    "    print(f\"\\nBest performing model: {best_model_name} with MAE = {mae_scores[best_model_name]:.4f}\")\n",
    "    \n",
    "    # Retrain the best model on the entire training data and make predictions on the validation set\n",
    "    predictions_val = train_and_predict_best_model(best_model_name, all_models, X_train, y_train, X_val)\n",
    "    \n",
    "    # If IDs are present in validation set, prepare submission\n",
    "    if ids_val is not None:\n",
    "        submission = pd.DataFrame({\n",
    "            ID_COLUMN: ids_val,\n",
    "            TARGET_COLUMN: predictions_val\n",
    "        })\n",
    "    else:\n",
    "        # If no ID column, create a simple index-based submission\n",
    "        submission = pd.DataFrame({\n",
    "            'Index': X_val.index,\n",
    "            TARGET_COLUMN: predictions_val\n",
    "        })\n",
    "    \n",
    "    # Save to CSV with specified encoding and newline characters\n",
    "    submission.to_csv(SUBMISSION_PATH, index=False, encoding='utf-8', line_terminator='\\n')\n",
    "    \n",
    "    print(f\"Submission file created at: {SUBMISSION_PATH}\")\n",
    "    \n",
    "    # Optional: Evaluate best model on validation set\n",
    "    mae = mean_absolute_error(y_val, predictions_val)\n",
    "    print(f\"Validation MAE for the best model ({best_model_name}): {mae:.4f}\")\n",
    "\n",
    "# Entry point\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loaded 24 files with total 1375028 rows.\n",
      "\n",
      "Preprocessing data...\n",
      "Numerical columns: ['WindSpeed(m/s)', 'Pressure(hpa)', 'Temperature(°C)', 'Humidity(%)', 'Sunlight(Lux)']\n",
      "Categorical columns: ['DateTime']\n",
      "\n",
      "Evaluating models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Linear Regression...\n",
      "Fold 1: Error = 38493432.573787495\n",
      "Fold 2: Error = 38020190.56843455\n",
      "Fold 3: Error = 38398581.81373827\n",
      "Fold 4: Error = 38391401.46303085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:  20%|██        | 1/5 [00:46<03:04, 46.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Error = 38537720.499942146\n",
      "Total Error for Linear Regression: 191841326.91893333\n",
      "\n",
      "Evaluating Random Forest...\n",
      "Fold 1: Error = 93718822.25417185\n",
      "Fold 2: Error = 91895136.2034235\n",
      "Fold 3: Error = 93544213.16002515\n",
      "Fold 4: Error = 93753100.68681169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:  40%|████      | 2/5 [01:21<01:59, 39.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Error = 91972034.9610286\n",
      "Total Error for Random Forest: 464883307.2654607\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "Fold 1: Error = 14636122.433047375\n",
      "Fold 2: Error = 14564105.524297822\n",
      "Fold 3: Error = 14549441.600261329\n",
      "Fold 4: Error = 14664425.828838393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:  60%|██████    | 3/5 [46:09<41:38, 1249.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Error = 14601152.008891726\n",
      "Total Error for Gradient Boosting: 73015247.39533664\n",
      "\n",
      "Evaluating LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.502835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2759\n",
      "[LightGBM] [Info] Number of data points in the train set: 1100022, number of used features: 802\n",
      "[LightGBM] [Info] Start training from score 248.661707\n",
      "Fold 1: Error = 13240652.759782262\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.529926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2901\n",
      "[LightGBM] [Info] Number of data points in the train set: 1100022, number of used features: 872\n",
      "[LightGBM] [Info] Start training from score 248.967439\n",
      "Fold 2: Error = 13082798.301974114\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.495775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2748\n",
      "[LightGBM] [Info] Number of data points in the train set: 1100022, number of used features: 796\n",
      "[LightGBM] [Info] Start training from score 249.016982\n",
      "Fold 3: Error = 13099793.760665204\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.505086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2775\n",
      "[LightGBM] [Info] Number of data points in the train set: 1100023, number of used features: 812\n",
      "[LightGBM] [Info] Start training from score 248.607772\n",
      "Fold 4: Error = 13207939.61848323\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.513161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2798\n",
      "[LightGBM] [Info] Number of data points in the train set: 1100023, number of used features: 824\n",
      "[LightGBM] [Info] Start training from score 248.877777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:  80%|████████  | 4/5 [47:06<12:58, 778.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Error = 13115304.849380925\n",
      "Total Error for LightGBM: 65746489.29028574\n",
      "\n",
      "Evaluating Decision Tree...\n",
      "Fold 1: Error = 13386080.44639697\n",
      "Fold 2: Error = 13284031.69955482\n",
      "Fold 3: Error = 13276346.722178057\n",
      "Fold 4: Error = 13365720.340492314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models: 100%|██████████| 5/5 [50:23<00:00, 604.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: Error = 13314794.54406684\n",
      "Total Error for Decision Tree: 66626973.752689004\n",
      "\n",
      "Model Performance Comparison:\n",
      "Model                    Total Absolute Error\n",
      "----------------------------------------\n",
      "LightGBM                 65746489.29\n",
      "Decision Tree            66626973.75\n",
      "Gradient Boosting        73015247.40\n",
      "Linear Regression        191841326.92\n",
      "Random Forest            464883307.27\n",
      "\n",
      "Results saved to 'model_comparison_results.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration and Constants\n",
    "# ----------------------------\n",
    "\n",
    "# Define file paths (Update these paths accordingly)\n",
    "TRAIN_DATA_DIR = './TrainingData'  # Replace with your training data directory path\n",
    "SUBMISSION_PATH = './submission.csv'  # Replace with your desired submission path\n",
    "\n",
    "# Define constants\n",
    "TARGET_COLUMN = 'Power(mW)'  # Replace with your target column name\n",
    "ID_COLUMN = 'LocationCode'    # Replace with your ID column name (if available)\n",
    "N_SPLITS = 5                   # Number of folds for cross-validation\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ----------------------------\n",
    "# Function Definitions\n",
    "# ----------------------------\n",
    "\n",
    "def load_training_data(train_dir):\n",
    "    \"\"\"\n",
    "    Load and concatenate all CSV files from the training directory.\n",
    "    \"\"\"\n",
    "    all_files = [os.path.join(train_dir, f) for f in os.listdir(train_dir) if f.endswith('.csv')]\n",
    "    if not all_files:\n",
    "        raise ValueError(f\"No CSV files found in the directory: {train_dir}\")\n",
    "    \n",
    "    df_list = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"Loaded {len(all_files)} files with total {combined_df.shape[0]} rows.\")\n",
    "    return combined_df\n",
    "\n",
    "def preprocess_data(df, target, id_col=None):\n",
    "    \"\"\"\n",
    "    Preprocess the data:\n",
    "    - Drop ID column if specified\n",
    "    - Separate features and target\n",
    "    - Identify numerical and categorical columns\n",
    "    - Create preprocessing pipeline\n",
    "    \"\"\"\n",
    "    if id_col and id_col in df.columns:\n",
    "        df = df.drop(columns=[id_col])\n",
    "    \n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    \n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "    \n",
    "    print(f\"Numerical columns: {numerical_cols}\")\n",
    "    print(f\"Categorical columns: {categorical_cols}\")\n",
    "    \n",
    "    # Define preprocessing steps for numerical and categorical data\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return X, y, preprocessor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def get_models():\n",
    "    \"\"\"\n",
    "    Define and return a dictionary of models to compare.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(\n",
    "            n_estimators=50,             # Reduced number of trees\n",
    "            max_depth=10,                # Limited tree depth\n",
    "            max_features='sqrt',         # Fewer features per split\n",
    "            min_samples_split=10,        # More samples required to split\n",
    "            min_samples_leaf=5,          # More samples per leaf\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1                    # Utilize all cores\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            random_state=RANDOM_STATE\n",
    "        ),\n",
    "        'LightGBM': LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            num_leaves=31,\n",
    "            max_depth=10,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Decision Tree': DecisionTreeRegressor(\n",
    "            max_depth=10, \n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    }\n",
    "    return models\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y, preprocessor, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluate a single model using K-Fold cross-validation and return the total absolute error.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    total_error = 0\n",
    "    fold = 1\n",
    "    \n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        # Create a pipeline with preprocessing and the model\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Fit the model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        \n",
    "        # Calculate absolute errors\n",
    "        abs_errors = np.abs(y_pred - y_val)\n",
    "        \n",
    "        # Sum the absolute errors as per competition scoring\n",
    "        fold_error = abs_errors.sum()\n",
    "        total_error += fold_error\n",
    "        \n",
    "        print(f\"Fold {fold}: Error = {fold_error}\")\n",
    "        fold += 1\n",
    "    \n",
    "    return total_error\n",
    "\n",
    "def main():\n",
    "    # Load the training data\n",
    "    print(\"Loading training data...\")\n",
    "    df = load_training_data(TRAIN_DATA_DIR)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    X, y, preprocessor = preprocess_data(df, target=TARGET_COLUMN, id_col=ID_COLUMN)\n",
    "    \n",
    "    # Get the models to evaluate\n",
    "    models = get_models()\n",
    "    \n",
    "    # Evaluate each model\n",
    "    print(\"\\nEvaluating models...\")\n",
    "    results = {}\n",
    "    for model_name, model in tqdm(models.items(), desc=\"Models\"):\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        total_error = evaluate_model(model, X, y, preprocessor, n_splits=N_SPLITS, random_state=RANDOM_STATE)\n",
    "        results[model_name] = total_error\n",
    "        print(f\"Total Error for {model_name}: {total_error}\")\n",
    "    \n",
    "    # Sort results by total error\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "    \n",
    "    # Display the comparison\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    print(\"{:<25}{}\".format(\"Model\", \"Total Absolute Error\"))\n",
    "    print(\"-\" * 40)\n",
    "    for model_name, error in sorted_results:\n",
    "        print(\"{:<25}{:.2f}\".format(model_name, error))\n",
    "    \n",
    "    # Optionally, save the results to a CSV file\n",
    "    results_df = pd.DataFrame(sorted_results, columns=['Model', 'Total_Absolute_Error'])\n",
    "    results_df.to_csv('model_comparison_results.csv', index=False)\n",
    "    print(\"\\nResults saved to 'model_comparison_results.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
